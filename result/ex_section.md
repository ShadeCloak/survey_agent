itle{Recent Advances in Code Reasoning and Generation with Large Language Models: A Survey}

\section{Program-Aided Language Models for Reasoning}

\subsection{Mathematical and Numerical Reasoning via Code}
%IDs:['1', '2', '3', '4', '13', '14', '17']
%In recent advancements in artificial intelligence and machine learning, the integration of programming logic into large language models (LLMs) has proven beneficial, particularly for complex mathematical reasoning tasks. Notably, GPT-4 Code Interpreter has shown remarkable performance by leveraging code assistance to enhance its mathematical reasoning abilities. This method, termed Direct Policy Optimization (DPO), directly optimizes policies using human preferences, bypassing the traditional two-phase approach involving utility learning and policy optimization \citep{rafailov2023direct}. 

DPO's differentiation lies in its ability to integrate human feedback directly for policy optimization, which simplifies the training process by eliminating the need for a learned reward function. This approach has been further enhanced by incorporating diverse divergence constraints, as demonstrated by \citet{wang2024reverse} \citep{wang2024reverse}. Similarly, SLiC-HF \citep{zhao2023slichf} addresses preference optimization by producing similar objectives to DPO. Despite these advancements, integrating LLMs with programming capabilities for mathematical reasoning tasks remains a challenge.

GPT-4 Code Interpreter excels at solving complex math problems by generating and executing code, evaluating the execution results, and self-correcting when necessary. This capability is bolstered by a novel method called Code-based Self-Verification (CSV), where the model verifies its answers using code and corrects them if erroneous \citep{kang2023reward}. 

Recent developments in datasets and fine-tuning methods for code-assisted LLMs have further improved their mathematical reasoning performance. MathCoder models, fine-tuned on datasets such as MATH and GSM8K, have significantly outperformed earlier models by generating code-based solutions for math problems \citep{liu2024chain}. Moreover, MuMath-Code combines tool-use LLMs with Python interpreters and data augmentation, achieving state-of-the-art performance on mathematical reasoning benchmarks \citep{wenhuchen2022program}.

While DPO and related approaches demonstrate potential, future research should explore methods to effectively leverage code assistance and self-correction in LLMs to tackle even more complex mathematical problems.\subsection{Program Generation and Execution for Enhanced Reasoning}
%IDs:['5', '6', '11', '12', '15', '16', '18']
%The evolution of code generation and execution through language models has witnessed significant advancements, predominantly driven by instruction tuning and reinforcement learning approaches. Instruction tuning, particularly, has become a cornerstone in enhancing the code-generation capabilities of LLMs, mirroring the success achieved in natural language processing tasks.

The seminal work of InstructGPT~\citep{2022InstructGPT} showcases the potential of training LLMs on a diverse set of tasks using instructions as prefixes, thereby enabling cross-task generalization. This methodology has been extended to code generation models, with notable contributions such as InstructCodeT5+~\cited{2023CodeT5+}, which utilizes 20K instruction data generated by InstructGPT. Similarly, StarCoder in WizardCoder~\citep{2023WizardCoder} and Pangu-Coder 2~\citep{2023Pangu-Coder2} employ instructions evolved from code Alpaca samples, either manually or through WizardLM, to improve code generation. These models underscore the versatility of instruction tuning in code generation, demonstrating enhancements across various code-related tasks.

Reinforcement learning (RL), another pivotal technique, complements instruction tuning by aligning the models with human values, particularly in contexts where non-differentiable reward signals, like compilers' feedback, are integral. CodeRL~\citep{2022CodeRL} exemplifies this synergy by training CodeT5 with reinforcement learning using token-level and program-level rewards. CompCoder~\citep{2022CompCoder} and PPOCoder~\citep{2023PPOCoder}, on the other hand, utilize proximal policy optimization to refine CodeGPT and CodeT5, respectively. RLTF~\citep{2023RLTF} introduces adaptive feedback based on compiler error information and test case ratios, further advancing RL's application in code generation.

Contrasting with these approaches, OctoCoder~\citep{2023OctoPack} innovates by using Git commit histories as instruction data, while CodeFuse~\citep{2023MFTCoder) integrates multitask finetuning and incorporates multiple downstream tasks into their instruction data. These models not only enrich the instruction data but also diversify the training paradigms, thereby broadening the applicability of LLMs in code generation.

In essence, instruction tuning and reinforcement learning have collectively propelled the frontier of LLM-based code generation. While instruction tuning broadens the model's ability to generalize across diverse code-related tasks, reinforcement learning aligns the models with human values, particularly in code execution and validation. Future research endeavors might explore integrating these methodologies with emerging technologies, such as natural language programming~\citep{2023WizardCoder}, to further democratize programming and enhance LLMs' code-generation prowess.\section{Code Integration and Structural Reasoning}

\subsection{Leveraging Code for Structured Reasoning and Generalization}
%IDs:['7', '9', '8', '19', '22', '23', '24']
%In the rapidly evolving landscape of artificial intelligence, the integration of code into natural language processing models has emerged as a promising avenue for enhancing computational tasks. Multiple studies have explored various techniques to infuse code-based methods into language models, thereby improving their performance and applicability in real-world scenarios. This section delves into three distinct yet complementary approaches employed in recent research to finesse code-related tasks.

Firstly, researchers have adopted the strategy of instruction fine-tuning and reinforcement learning for code-centric tasks (\citealp{2022InstructGPT}, \citealp{2022FLAN}, \citealp{2022OPT-IML}). This methodology involves training models on a diverse set of tasks with instruction prefixes, known as instruction fine-tuning, enabling these models to generalize across various tasks (\citealp{2022InstructGPT}, \citealp{2022FLAN}, \citealp{2022OPT-IML}). Initially, instruction data samples are manually compiled or crowd-sourced (\citealp{2021FLAN}, \citealp{2021T0}), but subsequent research reveals that LLM-generated instructions are equally effective (\citealp{2022Self-Instruct}, \citealp{2022Unnatural}). By extending these methods to the code community, models like InstructCodeT5+ (\citealp{2023CodeT5+}) and StarCoder (\citealp{2023WizardCoder}) have been fine-tuned with instruction data, showcasing significant performance enhancements.

Secondly, reinforcement learning (RL) has been employed to further refine code-related tasks (\citealp{2022CodeRL}, \citealp{2016RL-seq2seq}, \citealp{2017RL-human}). RL enables the incorporation of non-differentiable reward signals, such as compile errors and unit test failures, into the training process. Although human feedback is traditionally required for aligning language models, the use of compilers for code samples obviates this need, offering a natural advantage for RL in the code domain (\citealp{2022CodeRL}). Models like CodeRL (\citealp{2022CodeRL}), CompCoder (\citealp{2022CompCoder}), and PPOCoder (\citealp{2023PPOCoder}) exemplify this approach, utilizing RL techniques to optimize code generation and execution.

Lastly, the concept of direct policy optimization (DPO) has been explored as an alternative means of fine-tuning models with human feedback (\citealp{rafailov2023direct}, \citealp{zhao2023slichf}, \citealp{kang2023reward}, \citealp{an2023direct}, \citealp{song2024preference}, \citealp{liu2024statistical}). Unlike the two-phase approach involving utility learning and policy optimization, DPO directly employs preferences to optimize the policy, bypassing the need for reward learning. This method has shown promise in tasks such as preference-based policy optimization (\citealp{kang2023reward}), preference-aware policy optimization (\citealp{song2024preference}), and preference-based reinforcement learning with zeroth-order optimization (\citealp{tang2024zerothorder}).

In summary, the advent of instruction fine-tuning, reinforcement learning, and direct policy optimization has revolutionized the integration of code into natural language processing models. Each method offers unique advantages and addresses specific challenges in code-related tasks, thereby paving the way for more sophisticated and versatile AI systems.\subsection{Evaluation and Challenges in Code Reasoning with LLMs}
%IDs:['10', '25', '26', '20', '21', '27', '28']
%In the burgeoning field of Large Language Models (LLMs) and their applications in programmatic code and workflow generation, several innovative approaches have emerged, showcasing the potential to enhance both the expressiveness and usability of these models. Notably, FlowMind presents a novel method of utilizing LLMs, such as Generative Pretrained Transformers (GPT), to automatically generate workflows with high-level descriptions that can be easily understood and modified by users \citep{2024_FlowMind}. This framework addresses common issues faced by LLMs, including hallucinations, by ensuring data integrity through the elimination of direct interaction with proprietary code. FlowMind simplifies user interaction by presenting clear representations of auto-generated workflows, demonstrating the efficacy of its approach and underscoring the importance of user feedback in refining models \citep{2024_FlowMind}.

Similarly, AdaCoder introduces the concept of adaptive prompt compression to tackle the challenge of long input prompts required for programmatic visual question answering systems \citep{2024_AdaCoder}. By generating compressed preprompts tailored to specific question types, AdaCoder reduces token length while maintaining or improving VQA task performance. This adaptability across various LLM architectures without additional training is a significant advantage, showcasing the potential for enhancing VQA systems \citep{2024_AdaCoder}.

Building on compositional visual reasoning, PyramidCoder presents a multi-level prompting framework specifically designed for programmatic visual question answering tasks \citep{2024_PyramidCoder}. It comprises hierarchical levels for query rephrasing, code generation, and answer aggregation, managed by a single frozen LLM. This modular design not only eliminates the need for additional training but also ensures flexibility across different LLM architectures, leading to improved accuracy on various VQA benchmarks \citep{2024_PyramidCoder}.

Further delving into the intersection of LLMs and code execution, NExT proposes a method to teach LLMs to inspect and reason about code execution traces through a chain-of-thought rationale framework \citep{2024_NExT}. This method enhances LLMs' abilities to debug and repair code, significantly improving performance on program repair tasks. NExT's ability to generalize to scenarios without program traces during testing underscores its robustness and potential to revolutionize code understanding and reasoning by LLMs \citep{2024_NExT}.

In summary, these advancements in LLM capabilities for code generation and execution through adaptable prompts, hierarchical frameworks, and programmatic reasoning represent significant strides in enhancing the practicality and efficiency of LLMs in software development tasks. These techniques not only expand the scope of what LLMs can achieve but also highlight the ongoing evolution of AI in programming and software engineering.\section{Interactive and Autonomous Code Generation}

\section{Interactive and Autonomous Code Generation}

\subsection{Interactive and Multi-Agent Code Generation Systems}
%IDs:['29', '30', '31', '32', '33', '34', '38']
%In recent advancements in automated code generation, researchers have explored various frameworks to harness the capabilities of Large Language Models (LLMs). One such framework is the self-collaboration approach, where multiple LLM agents work in unison to tackle complex coding tasks. The ChatGPT-powered system introduced by \citet{lu2023self} exemplifies this, assigning distinct roles such as analyst, coder, and tester to different agents, facilitating cooperative code generation without human intervention. This framework not only improves solution coherence but also enhances LLM efficiency in managing complex repository-level tasks, showcasing a 29.9% to 47.1% improvement in Pass@1 metrics compared to single-agent models (\citealp{lu2023self}).

Another notable development is the ChatDev framework, designed by \citet{wang2023chatdev}, which integrates specialized agents driven by LLMs to facilitate communicative and coordinated software development across design, coding, and testing phases. This unified language-based approach enhances collaboration and reduces technical inconsistencies, illustrating the potential of linguistic communication in enabling multi-agent task-solving.

The MetaGPT framework proposed by \citet{kulick2023metagpt} further advances this collaboration by encoding standardized operating procedures into prompt sequences, reducing errors and enhancing workflow efficiency. This innovative meta-programming approach allows agents to verify intermediate results, thereby generating more coherent solutions for complex problems.

The CodeChain framework, introduced by \citet{sampson2023codechain}, addresses the challenge of generating modularized code by iteratively refining and reusing sub-modules. This method significantly improves both modularity and solution correctness, achieving 35% relative pass@1 improvements on APPS and 76% on CodeContests (\citealp{sampson2023codechain}).

Furthermore, the CodeAgent framework by \citet{gao2023codeagent} integrates external programming tools to aid LLMs in real-world repo-level tasks, enhancing performance by up to 250% over existing models (\citealp{gao2023codeagent}). This demonstrates the potential of tool-integrated agent systems in addressing complex coding challenges.

In summary, these advances in LLM-based collaborative frameworks highlight the significant strides made in automated code generation. By leveraging multi-agent systems, unified communication, and modular approaches, these frameworks not only improve code quality but also extend the capabilities of LLMs in tackling complex and real-world coding tasks.\subsection{Autonomous Code Generation and Self-Improvement}
%IDs:['35', '36', '37', '39', '40', '41', '42', '43']
%The advent of Large Language Models (LLMs) has revolutionized various domains, including software development, with their ability to understand and generate code. This has led to the creation of numerous frameworks leveraging LLMs for different software engineering tasks. This section delves into several notable works that employ multiple LLM agents for code generation, program repair, and GitHub issue resolution.

\subsection{Code Generation Frameworks}
One notable work is LCG (\citealp{2403.15852}), which introduces a code generation framework inspired by software engineering practices. LCG incorporates multiple LLM agents to emulate various software process models such as Waterfall, TDD, and Scrum. Each model assigns specific roles to LLM agents, mirroring typical development activities. Through collaborative efforts and prompt composition, the agents refine their code continuously. Evaluated across four benchmarks, LCGScrum outperforms other models, showcasing a significant improvement over baseline GPT.

Another groundbreaking work is MapCoder (\citealp{2405.11403}), which employs a multi-agent prompting approach to replicate the full cycle of program synthesis observed in human developers. MapCoder consists of four LLM agents designed to handle different stages of the cycle: recalling relevant examples, planning, code generation, and debugging. This framework achieves state-of-the-art performance across various programming languages and problem difficulties.

\subsection{Program Repair}
RepairAgent (\citealp{2403.17134}) represents the first autonomous, LLM-based agent for program repair. Unlike existing approaches, RepairAgent treats the LLM as an agent capable of autonomously planning and executing actions to fix bugs. It interacts with suitable repair tools based on gathered information and feedback from previous attempts. The agent’s effectiveness is demonstrated on the Defects4J dataset, where it successfully repairs 164 bugs, including 39 previously unresolved ones.

\subsection{GitHub Issue Resolution}
MAGIS (\citealp{2403.17927}) presents a novel LLM-based Multi-Agent framework for resolving GitHub issues. It addresses the challenges faced by LLMs in resolving issues at the repository level by employing four specialized agents: Manager, Repository Custodian, Developer, and Quality Assurance Engineer. Through collaborative planning and coding, MAGIS significantly outperforms popular LLMs like GPT-3.5, GPT-4, and Claude-2 in resolving GitHub issues.

\subsection{AutoCodeRover}
AutoCodeRover (\citealp{2404.05427}) focuses on program improvement through automated bug fixing and feature additions. It combines LLMs with sophisticated code search capabilities to achieve program modifications or patches. By working on a program representation (abstract syntax tree), AutoCodeRover enhances LLM understanding of the issue’s root cause, leading to effective context retrieval and bug resolution.

These frameworks illustrate the potential of combining LLMs with software engineering practices to tackle complex tasks. Each method presents unique strengths and addresses different aspects of software development, from code generation and program repair to issue resolution. Future research can explore further integration and optimization of LLMs in the software development process.

\section{Refinement, Optimization, and Benchmarking in Code Generation}

\subsection{Iterative Refinement and Self-Optimization}
%IDs:['53', '54', '55', '57', '59', '60', '61', '63']
%The emergence of large language models (LLMs) as versatile tools for code generation has spurred innovation in self-optimization techniques, particularly in the iterative enhancement and error correction of generated code. These methods aim to ensure the models can refine their outputs without relying on explicit human feedback or additional training data.

One significant method developed is Self-Refine, introduced by \citet{self_refine}. It leverages the iterative feedback mechanism LLMs use to refine text, applying this to code generation. Through this approach, an LLM generates an initial code snippet which then serves as the basis for subsequent refinement stages, where feedback from the same LLM is used to improve the code. Notably, this process requires no supervised training data, additional training, or reinforcement learning, utilizing the LLM solely for generation, feedback, and refinement. Evaluation across various tasks shows this method significantly enhances the initial code quality, improving task performance by approximately 20% on average.

Another method, Self-Debugging, as proposed by \citet{self_debugging}, teaches LLMs to debug their own code. This self-learning process involves demonstrating the model with examples of debugging scenarios and allowing it to identify and correct errors by analyzing execution results. It excels in improving code generation performance on complex programming tasks and notably enhances the model's sample efficiency by reusing failed predictions.

The Self-Edit approach, devised by \citet{self_edit}, incorporates execution results into the code generation process. By executing the generated code and using the results to guide editing, it effectively corrects errors, especially on competitive programming tasks. It enhances the pass@1 rate by 89% on APPS-dev, 31% on APPS-test, and 48% on HumanEval, showcasing the benefit of integrating execution feedback into the code generation process.

Further refining the concept of self-correction, the CYCLE framework, introduced by \citet{cycle}, focuses on improving the self-refinement capability of LLMs. It leverages feedback from execution results to improve faulty generations, leading to a consistent improvement in code quality, especially in scenarios where initial code predictions are incorrect.

These self-optimization methods collectively demonstrate the potential for LLMs to enhance their own code generation capabilities through iterative refinement and self-correction. While each method presents unique advantages and scenarios where it performs best, they collectively signal a promising direction in the development of more autonomous and self-improving code generation models.\subsection{Benchmarking and Evaluation of LLM-Based Code Generation}
%IDs:['58', '62', '44', '45', '46', '47', '48', '49']
%The advent of advanced frameworks and methodologies to enhance LLMS-based code generation has led to notable developments. Such innovations have significantly improved the integration, safety, and effectiveness of LLMs in code generation. This paper will discuss some of the prominent approaches that have been introduced to enhance the capabilities of LLMs in software development.

WizardCoder presents a noteworthy development by leveraging the methods of WizardLM to generate instruction data, thereby refining the performance of code generation models. This model expands the dataset and integrates advanced techniques such as reinforcement learning, as seen in Pangu-Coder 2, which aligns code generation with objective outcomes through Test & Teacher Feedback \cite{2023WizardCoder}. In comparison, OctoCoder distinguishes itself by harnessing Git commit histories as instructional data, refining models like StarCoder and CodeGeeX2. It offers unique insights into the evolution of coding methodologies by bridging historical and contemporary practices \cite{2023OctoPack}.

The CodeFuse framework employs multitask finetuning to incorporate multiple downstream tasks into instruction data, thereby enriching the learning experience and improving code generation accuracy. By introducing diverse tasks, it underscores the importance of task exposure in driving model enhancements. In a similar vein, the AgileCoder model presents a multi-agent system that integrates Agile Methodology. This results in more efficient task completion and accurate code generation \cite{2023AgileCoder}. 

The application of reinforcement learning from human feedback (RLHF) in code generation, exemplified by models like CodeRL, also marks a significant advancement. This strategy structures reward systems based on compile errors, runtime errors, and unit test outcomes, ensuring functional and performance alignment \cite{2022CodeRL}. 

CodeNav introduces a novel approach by enabling LLM agents to navigate and leverage real-world code repositories to solve user queries. This method distinguishes itself by indexing and searching code blocks within the target codebase, leading to iterative solution generation with execution feedback \cite{2023CodeNav}.

Further, INDICT proposes a framework that incorporates internal dialogues of critiques for safety and helpfulness guidance. By engaging a dual critic system, it provides preemptive and post-hoc guidance, markedly improving code quality \cite{2024INDICT}. 

Together, these methodologies demonstrate a convergent evolution in LLM-based code generation. They merge diverse data sources, advanced techniques, and real-world applications to enhance code accuracy, safety, and helpfulness, thereby bridging the gap between academic theory and practical application.\subsection{Advanced Techniques in Code Repair and Generation}
%IDs:['64', '65', '66', '67', '50', '51', '52', '56', '68']
%This section delves into the innovative paradigm of instruction-tuning for code generation and explores the synergy of reinforcement learning techniques within this framework, focusing on the distinct approaches and their comparative impacts on LLM-based code generation models. This discussion is structured to reflect the rigorous and methodical standards adhered to in academic writing.

Notably, researchers leveraging instruction-tuning in natural language processing have achieved remarkable success in enhancing model generalization across various tasks through diverse instruction datasets (\citealp{2022InstructGPT}; \citealp{2022FLAN}; \citealp{2022OPT-IML}). Such instruction-tuning approaches involve training models on a broad array of tasks with instructive prefixes, which notably requires manual compilation or crowdsourcing of instruction data samples (\citealp{2021FLAN}; \citealp{2021T0}). Nonetheless, this methodology has since been refined, as recent studies suggest that LLM-generated instructions suffice (\citealp{2022Self-Instruct}; \citealp{2022Unnatural}).

Transcending from natural language to code generation, notable endeavors include the development of InstructCodeT5+ by finetuning CodeT5+ with 20,000 instruction data samples sourced from InstructGPT (\citealp{2023CodeT5+}), and StarCoder enhanced through WizardCoder, following the methods of WizardLM (\citealp{2023WizardCoder}). Pangu-Coder 2 (\citealp{2023Pangu-Coder2}) further innovates by generating instruction samples via WizardLM and integrating reinforcement learning via the Rank Responses to Align Test & Teacher Feedback (RRTF) mechanism. Meanwhile, OctoCoder (\citealp{2023OctoPack}) diverges by utilizing Git commit histories to finetune StarCoder and CodeGeeX2. Moreover, CodeFuse (\citealp{2023MFTCoder}) adopts multitask-finetuning and introduces multiple downstream tasks into their instruction data.

Concurrent with instruction-tuning, reinforcement learning from human feedback (RLHF) emerges as a pivotal technique in the field of natural language processing, significantly enhancing LLM alignment with human values and preferences (\citealp{2022InstructGPT}; \citealp{2022Anthropic}). This method incorporates non-differentiable reward signals from human evaluation into training models, though it requires extensive labor for annotation. Conversely, the realm of code generation benefits from RL as automated feedback from compilers can be easily integrated into the training process.

A notable advancement in this regard is CodeRL (\citealp{2022CodeRL}), which uses compiler feedback to define rewards for program compilation status. Theactor model, an extended version of CodeT5, is then trained with the REINFORCE algorithm, significantly boosting model performance. Similarly, CompCoder (\citealp{2022CompCoder}) and PPOCoder (\citealp{2023PPOCoder}) apply proximal policy optimization (PPO) to fine-tune CodeGPT and CodeT5 respectively. Moreover, RLTF (\citealp{2023RLTF}) proposes fine-grained feedback derived from error information and locations provided by compilers, in addition to adaptive feedback considering test case ratios.

These pioneering approaches underscore the evolving landscape of LLM-based code generation, with instruction-tuning and reinforcement learning representing cornerstones of advancement. As research progresses, finer-tuned and more adaptive models become feasible, propelling the field towards unprecedented efficiency and accuracy in code generation tasks. The performance enhancements in LLM-based models underscore the vast potential of this approach, setting a formidable precedent for future research in software development and natural language processing.