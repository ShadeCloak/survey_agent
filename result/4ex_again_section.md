\subsection{Evaluation and Challenges in Code Reasoning with LLMs}
%IDs:['10', '25', '26', '20', '21', '27', '28']
%The rapid advancements in the field of Robotic Process Automation (RPA) have revolutionized the automation of repetitive tasks, yet their effectiveness is limited in scenarios requiring spontaneous or unpredictable user demands. \citet{2024FlowMind} address this limitation with FlowMind, a system that leverages Large Language Models (LLMs) to create an automatic workflow generation system. flowmind mitigates the common issue of hallucinations in LLMs by grounding its reasoning with reliable Application Programming Interfaces (APIs), ensuring the integrity and confidentiality of information, especially crucial in financial services. The system also improves user interaction by presenting high-level descriptions of auto-generated workflows, facilitating effective inspection and feedback. Their evaluations demonstrate the success of FlowMind, underlining the importance of each component in the proposed system and the effectiveness of user interaction and feedback. Furthermore, they introduce NCEN-QA, a new dataset in finance for benchmarking question-answering tasks from N-CEN reports on funds, showcasing the applicability and performance of FlowMind in real-world scenarios.

In a similar vein, \citet{2024AdaCoder} tackle the challenge of long input prompts in Visual Programmatic Models (VPMs) by introducing AdaCoder, an adaptive prompt compression framework. AdaCoder operates in two phases: a compression phase that generates compressed preprompts based on specific question types, and an inference phase where the appropriate preprompt is selected to generate code. This framework significantly reduces token length by 71.1% while maintaining or even improving the performance of visual question answering, showcasing its adaptability and efficiency.

Similarly, \citet{2024PyramidCoder} introduce PyramidCoder, a hierarchical code generator for compositional Visual Question Answering (VQA). PyramidCoder improves VQA accuracy by leveraging a single frozen LLM and pre-defined prompts across three hierarchical levels, demonstrating its flexibility and state-of-the-art performance on various VQA datasets.

On the topic of code simulation, \citet{2024DualChainsOfLogic} explore the novel task of logic code simulation, where LLMs emulate logical solvers to predict the results of logical programs. Their introduced Dual Chains of Logic (DCoL) technique enhances LLM performance by advocating a dual-path thinking approach, achieving a notable improvement in accuracy with GPT-4-Turbo.

Furthermore, \citet{2024REval} address the limitations of existing code reasoning benchmarks by proposing REval, a framework for evaluating code reasoning abilities and consistency with program execution. Their large-scale empirical study reveals the urgent need to strengthen the code reasoning capability of code LLMs, reflecting unsatisfactory performance across both Runtime Behavior Reasoning and Incremental Consistency Evaluation.

Lastly, \citet{2024SelfPiCo} introduce SelfPiCo, a framework that dynamically guides partial code execution by incorporating an open-source LLM within an interactive loop. SelfPiCo leverages few-shot in-context learning and chain-of-thought reasoning to continuously learn and refine predictions, outperforming existing tools and showcasing its practical usage and potential applications in software debugging and testing.